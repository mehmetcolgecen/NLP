{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[323906397735641088, 323906483584655360, 323906657333682176, 323907258301939713, 323909308188344320, 323913403460636673, 324067437886713856, 324117950774775809, 324138055772561408, 324219503401644033, 324320247018573824, 324346553835868161, 324372750330363904, 324408472441585664, 324422817565257728, 324448013999304704, 324785120085176320, 325059351209443329, 325060324992643072, 325162944931438592, 325253327048822784, 325337623910559745, 325409910642835456, 325701934273134594, 325946633986641920]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7a637a852789>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mterms_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output2' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "\n",
    "#intial data cleaning and preprocessing using regular expressions\n",
    "\n",
    "regex_str = [\n",
    "     r'<[^>]+>', \n",
    "    r'(?:@[\\w_]+)', \n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', \n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", \n",
    "    r'(?:[\\w_]+)', \n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# defining the Jaccard distance\n",
    "def jaccard(a,b):\n",
    "    inter= list(set(a) & set(b))\n",
    "    I=len(inter)\n",
    "    union= list(set(a) | set(b))\n",
    "    U=len(union)\n",
    "    return round(1-(float(I)/U),4)\n",
    "\n",
    "\n",
    "\n",
    "#k-means implementation\n",
    "def kmeans(id,centroids,terms_all,l,k):\n",
    "\n",
    "    count=0\n",
    "    for h in range(25):\t\n",
    "        count=count+1\n",
    "        indices=[id.index(item) for item in centroids]\n",
    "        cen_txt=[ terms_all[x] for x in indices]\n",
    "        cluster=[]\n",
    "        for i in range(l):\n",
    "            d=[jaccard(terms_all[i],cen_txt[j]) for j in range(k)]\n",
    "            ans= d.index(min(d))\n",
    "            cluster.append(ans)\n",
    "            \n",
    "        centroids1=up_date(id,cluster,terms_all,l,k)\n",
    "        sum=0\n",
    "        for i in range(k):\n",
    "            if (centroids1[i]==centroids[i]):\n",
    "                sum=sum+1\n",
    "        if (sum==k):\n",
    "            break;   \n",
    "        centroids = copy.deepcopy(centroids1)\n",
    "\n",
    "    output(cluster,k,id)\n",
    "    sse(cluster,centroids,terms_all,k,l)\n",
    "    f.close()\n",
    "\n",
    "#output in the form of cluster# and the array of the tweetids in that cluster.\n",
    "\n",
    "def output(cluster,k,id):\n",
    "        final=[]\n",
    "        for i in range(k):\n",
    "            final.append([j for j, u in enumerate(cluster) if u == i])\n",
    "            t=[x for x in final[i]]\n",
    "            print >> f , i+1,[id[x] for x in t]\n",
    "\n",
    "#computing the sum of squared errors\n",
    "\n",
    "def sse(cluster,centroids,terms_all,k,l):\n",
    "    indices1=[]\n",
    "    indices=[id.index(item) for item in centroids]\n",
    "    cen_txt=[ terms_all[x] for x in indices]\n",
    "    sum=0\n",
    "    for i in range(k):\n",
    "        indices1.append([j for j, u in enumerate(cluster) if u == i])\n",
    "        #print indices1[i]\n",
    "        t= [ terms_all[x] for x in indices1[i]]\n",
    "        for j in range(len(indices1[i])):\n",
    "            sum=sum + math.pow(jaccard(t[j],cen_txt[i]),2)\n",
    "    print >> f, 'SSE',sum\n",
    "\n",
    "\n",
    "# updatin gthe centroids at every iteration\n",
    "def up_date(id,cluster,terms_all,l,k):\n",
    "    indices=[]\n",
    "    new_centxt_index=[]\n",
    "    new_centroid=[]\n",
    "    for i in range(k):\n",
    "        indices.append([j for j, u in enumerate(cluster) if u == i])\n",
    "        m=indices[i]\n",
    "        #print m\n",
    "        #m gives the indices if the elements of every cluster k\n",
    "\n",
    "        if (len(m) != 0):\n",
    "            txt=[terms_all[p] for p in m]\n",
    "            sim =[ [jaccard(txt[i],txt[j]) for j in range(len(m))] for i in range(len(m))]\n",
    "            #print sim\n",
    "    #symmetric distance matrix\n",
    "            #print [sum(i) for i in sim]\n",
    "            f1=[sum(i) for i in sim]\n",
    "            #print f.index(min([sum(i) for i in sim]))\n",
    "    #lower triangular matrix\n",
    "            new_centxt_index.append( m[(f1.index(min([sum(i) for i in sim])))]) #index of the point closer to all the other points\t\n",
    "    new_centroid=[id[x] for x in new_centxt_index]\n",
    "    return new_centroid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "terms_all=[]\n",
    "id=[]\n",
    "with open(\"Tweets.json\", 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        t=tweet['text']\n",
    "        \n",
    "        tokens = preprocess(t)\n",
    "        terms_all.append([term for term in tokens])\n",
    "        \n",
    "        d=tweet['id']\n",
    "        id.append(d)\n",
    "        \n",
    "\n",
    "l=len(terms_all)\n",
    "text_file = open(\"InitialSeeds.txt\", \"r\")\n",
    "centroids = text_file.read().split(',')\n",
    "centroids= [x.strip('\\n') for x in centroids]\n",
    "centroids= [int(x) for x in centroids]    #ids of centroids\n",
    "print(centroids)\n",
    "k=len(centroids)\n",
    "f = open(str(output2.txt), 'w')\n",
    "kmeans(id,centroids,terms_all,l,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
