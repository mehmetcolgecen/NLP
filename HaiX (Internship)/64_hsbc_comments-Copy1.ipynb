{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import spacy\n",
    "import string\n",
    "import hdbscan\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "ek3w6OZQsoqv",
    "outputId": "92306d37-cc10-4cb7-c62e-006de9eae451"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hsbc_comments.csv')\n",
    "df.rename(columns={\"message\":\"comments\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Applying tweet-preprocesser library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notice how they havent replied to anyone?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.clean(\"Notice how they haven\\u2019t replied to anyone?\\U0001f923\\U0001f923\\U0001f923 https://google.com #hello @mehmet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.comments.replace(\"\\n\" , \" \", regex=True, inplace = True)\n",
    "df.comments=df.comments.apply(lambda x:p.clean(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.comments=df.comments.apply(lambda x:p.clean(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Applying SpsCy lemmatizer, isalpha() and removing short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def word_root(text):\n",
    "    list_data3=[]\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.isalpha():\n",
    "            lemma = token.lemma_\n",
    "            if len(lemma)>2:\n",
    "                list_data3.append(lemma)\n",
    "    return \" \".join(list_data3)\n",
    "\n",
    "df.comments = df.comments.apply(lambda x: word_root(str(x).lower()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.comments = df.comments.apply(lambda x: word_root(str(x).lower()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice how they have reply anyone'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_root(p.clean(\"Notice how they haven\\u2019t replied to anyone?\\U0001f923\\U0001f923\\U0001f923 https://google.com #hello @mehmet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Removing ROWS which has NaN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('nan', np.nan, inplace = True)\n",
    "df.replace('', np.nan, inplace = True)\n",
    "df.replace('do', np.nan, inplace = True)\n",
    "df = df.dropna()\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = df.comments.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model, Dimension Reduction and Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##     a. BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "6b163e4049ca4cbe8a24e7ac79ced8a5",
      "be0f028285fa4e8da720237e1b1279bf",
      "b02fccb0afa047999665079b5248015f",
      "10748c34bb5448d9a34cd200a3979d56",
      "b927e6a6b1c44be1be1f58d582da2f6a",
      "ea4553e744634f56a3128b209cac5330",
      "39fd5938a4df4de2891cd8393dd7af2f",
      "7e2aa975791049c8a7f588ed76b379c0"
     ]
    },
    "id": "BjwnhvRQtW2S",
    "outputId": "458d7bd4-ba13-4112-85a9-fb050e99008f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model=SentenceTransformer('distilbert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7823907e824f3096ab438dcc89689d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = model.encode(list_data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. UMAP Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "g8zUQEqTta2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = umap.UMAP(n_components=5,min_dist=0.0).fit_transform(embeddings)\n",
    "\n",
    "# #n_neighbors=10, min_dist=0.0, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load(\"HSBC_comments_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering and Reducing Clusters with Cosine-Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 88.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cluster = KMeans(n_clusters=5, init=\"k-means++\").fit_predict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cluster = hdbscan.HDBSCAN().fit_predict(best_model)\n",
    "KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>call the evening the suggestion but but they h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>melanie humberstone brilliant they card take l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>imagination run wild wild wild try get your ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>would love donate again this year year unfortu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>would like say massive thank you hsbc and the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                           comments\n",
       "0        0  call the evening the suggestion but but they h...\n",
       "1        1  melanie humberstone brilliant they card take l...\n",
       "2        2  imagination run wild wild wild try get your ba...\n",
       "3        3  would love donate again this year year unfortu...\n",
       "4        4  would like say massive thank you hsbc and the ..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs = pd.DataFrame(list_data,columns=[\"comments\"])\n",
    "docs[\"cluster\"] = cluster\n",
    "labeled_docs = docs.groupby([\"cluster\"], as_index=False).agg({\"comments\": \" \".join})\n",
    "labeled_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>would like say massive thank you hsbc and the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imagination run wild wild wild try get your ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you would send the neccesary number would use ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just past hour wait again again for the time t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>melanie humberstone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brilliant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>absolutely fume fume sit hold for almost hour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>would love donate again this year year unfortu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>possible you can answer telephone less than hr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>they card take long opne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have have the bad banking experience life life...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>complaint complaint complaint bad team all tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>call the evening the suggestion but but they h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>notice how they they reply</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yes yes good you you try make online payment b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>why you not deal with money steal from persona...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>begin believe you not want new business busine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>glad introduce you this genuine company earn c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>scam scam scam really your bank scam have have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>need sort out bounce back loan not cycle event...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  cluster\n",
       "0   would like say massive thank you hsbc and the ...        1\n",
       "1   imagination run wild wild wild try get your ba...        1\n",
       "2   you would send the neccesary number would use ...        1\n",
       "3   just past hour wait again again for the time t...        1\n",
       "4                                 melanie humberstone        0\n",
       "5                                           brilliant        0\n",
       "6   absolutely fume fume sit hold for almost hour ...        1\n",
       "7   would love donate again this year year unfortu...        1\n",
       "8   possible you can answer telephone less than hr...        1\n",
       "9                            they card take long opne        0\n",
       "10  have have the bad banking experience life life...        1\n",
       "11  complaint complaint complaint bad team all tim...        1\n",
       "12  call the evening the suggestion but but they h...        1\n",
       "13                         notice how they they reply        0\n",
       "14  yes yes good you you try make online payment b...        1\n",
       "15  why you not deal with money steal from persona...        1\n",
       "16  begin believe you not want new business busine...        1\n",
       "17  glad introduce you this genuine company earn c...        1\n",
       "18  scam scam scam really your bank scam have have...        1\n",
       "19  need sort out bounce back loan not cycle event...        1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Merging Similar Clusters with Cosine-Similarity (Applied 2 times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "array_text = labeled_docs.comments.tolist()\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentences = array_text[1:]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "similar = []\n",
    "for score, i, j in all_sentence_combinations:\n",
    "#     print(\"cluster {} \\t cluster {} \\t similarity: {:.4f}\".format(i+1, j+1, cos_sim[i][j]))\n",
    "    similar.append((score,i+1,j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 166 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "while similar[i][0]>0.40:\n",
    "    one, two = sorted([similar[i][1],similar[i][2]])\n",
    "    if labeled_docs.comments[one] == \"\" and labeled_docs.comments[two] == \"\":\n",
    "        pass\n",
    "    elif labeled_docs.comments[one] == \"\" and labeled_docs.comments[two] != \"\":\n",
    "        labeled_docs.comments[one] = labeled_docs.comments[two]\n",
    "    else:\n",
    "        labeled_docs.comments[one] = labeled_docs.comments[one] +\" \"+labeled_docs.comments[two]\n",
    "    labeled_docs.comments[two]=\"\"\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_docs.replace('', np.nan, inplace = True)\n",
    "labeled_docs=labeled_docs.dropna()\n",
    "labeled_docs.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "array_text = labeled_docs.comments.tolist()\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentences = array_text[1:]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "similar = []\n",
    "for score, i, j in all_sentence_combinations:\n",
    "#     print(\"cluster {} \\t cluster {} \\t similarity: {:.4f}\".format(i+1, j+1, cos_sim[i][j]))\n",
    "    similar.append((score,i+1,j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "while similar[i][0]>0.40:\n",
    "    one, two = sorted([similar[i][1],similar[i][2]])\n",
    "    if labeled_docs.comments[one] == \"\" and labeled_docs.comments[two] == \"\":\n",
    "        pass\n",
    "    elif labeled_docs.comments[one] == \"\" and labeled_docs.comments[two] != \"\":\n",
    "        labeled_docs.comments[one] = labeled_docs.comments[two]\n",
    "    else:\n",
    "        labeled_docs.comments[one] = labeled_docs.comments[one] +\" \"+labeled_docs.comments[two]\n",
    "    labeled_docs.comments[two]=\"\"\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_docs.replace('', np.nan, inplace = True)\n",
    "labeled_docs=labeled_docs.dropna()\n",
    "labeled_docs.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Keyword Extruction with KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   Bigram Cluster 1: \u001b[0m\n",
      "[('horrendous bank', 0.5735), ('bank disgusting', 0.5733), ('disgusting bank', 0.5686), ('terrible customer', 0.5369), ('terrible bank', 0.5368), ('awful bank', 0.534), ('terrible hsbc', 0.5285), ('bank misfortune', 0.5268), ('dreadful customer', 0.5056), ('hsbc disgusted', 0.5021)]\n",
      "\u001b[1m   Bigram Cluster 2: \u001b[0m\n",
      "[('love colleague', 0.5079), ('amazing love', 0.4717), ('love quote', 0.4372), ('proud nina', 0.427), ('love love', 0.427), ('passion love', 0.421), ('love word', 0.4177), ('happy diwali', 0.4136), ('gallacher amazing', 0.4109), ('beautiful service', 0.4105)]\n",
      "\u001b[1m   Bigram Cluster 3: \u001b[0m\n",
      "[('tuesday application', 0.4406), ('tuesday tuesday', 0.4093), ('following tuesday', 0.3688), ('today tuesday', 0.3675), ('tuesday january', 0.3643), ('hsbc backlog', 0.361), ('husband facebook', 0.3605), ('facebook facebook', 0.3578), ('hsbc twitter', 0.3566), ('new treasurer', 0.3497)]\n",
      "\u001b[1m   Bigram Cluster 4: \u001b[0m\n",
      "[('diabetes diabetes', 0.3337), ('medicine weekend', 0.3158), ('greedy banker', 0.3073), ('netflix hsbc', 0.304), ('watch netflix', 0.2895), ('massage facebook', 0.2829), ('hsbc instagram', 0.2682), ('bank fraud', 0.2682), ('millionaire millionaire', 0.2663), ('great tuesday', 0.2663)]\n",
      "\u001b[1m   Bigram Cluster 5: \u001b[0m\n",
      "[('hsbc thank', 0.4181), ('efficient bank', 0.4079), ('hsbc app', 0.4033), ('thank hsbc', 0.4025), ('banking app', 0.3895), ('ceo wish', 0.3771), ('hsbc phone', 0.3661), ('hsbc saving', 0.3574), ('banking fit', 0.3567), ('banking phone', 0.3527)]\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keybert import KeyBERT\n",
    "array_text = labeled_docs.comments.tolist()\n",
    "kw_extractor = KeyBERT(model=model)\n",
    "bigram=[]\n",
    "for j in range(len(array_text)):\n",
    "    keywords = kw_extractor.extract_keywords(array_text[j],top_n=10,keyphrase_ngram_range=(2, 2))\n",
    "    print(f'\\033[1m   Bigram Cluster {j+1}: \\033[0m' )\n",
    "    print([word for word in keywords])\n",
    "    bigram.append([word[0] for word in keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   Unigram Cluster 1: \u001b[0m\n",
      "[('horrendous', 0.3536), ('racist', 0.3363), ('bankrupt', 0.3259), ('fraudster', 0.3109), ('incompetent', 0.2908), ('disgusting', 0.2817), ('pandemic', 0.2797), ('disgusted', 0.2791), ('disgraceful', 0.2769), ('sick', 0.2757)]\n",
      "\u001b[1m   Unigram Cluster 2: \u001b[0m\n",
      "[('love', 0.2959), ('happy', 0.2844), ('skyla', 0.2813), ('proud', 0.2746), ('grateful', 0.2696), ('merry', 0.2264), ('beautiful', 0.2182), ('thank', 0.2084), ('aplaude', 0.1961), ('passion', 0.1853)]\n",
      "\u001b[1m   Unigram Cluster 3: \u001b[0m\n",
      "[('tuesday', 0.3282), ('facebook', 0.2335), ('june', 0.2175), ('thursday', 0.2141), ('payday', 0.2069), ('emailing', 0.2053), ('birthday', 0.1853), ('monday', 0.1841), ('christmas', 0.1825), ('google', 0.1815)]\n",
      "\u001b[1m   Unigram Cluster 4: \u001b[0m\n",
      "[('tuesday', 0.2112), ('diabetes', 0.1935), ('netflix', 0.1803), ('millionaire', 0.1235), ('heartbroken', 0.1214), ('instagram', 0.1016), ('phishing', 0.0977), ('plague', 0.0966), ('ebay', 0.095), ('june', 0.0898)]\n",
      "\u001b[1m   Unigram Cluster 5: \u001b[0m\n",
      "[('hsbc', 0.1657), ('facebook', 0.163), ('ceo', 0.159), ('banking', 0.153), ('wallet', 0.1487), ('mortgage', 0.1451), ('email', 0.1361), ('password', 0.1161), ('bank', 0.1053), ('twitter', 0.1051)]\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unigram=[]\n",
    "for j in range(len(array_text)):\n",
    "    keywords = kw_extractor.extract_keywords(array_text[j],top_n=10,keyphrase_ngram_range=(1, 1))\n",
    "    print(f'\\033[1m   Unigram Cluster {j+1}: \\033[0m' )\n",
    "    print([word for word in keywords])\n",
    "    unigram.append([word[0] for word in keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tejas shinde\n",
    "1. Using preprocessing library to clean - removing emoji url @mentions and #mentions\n",
    "2. function to lemmatize and lower which removes symbols(checking alpha)\n",
    "3. removing documents which are NA and do \n",
    "4. loading BERT model\n",
    "5. embeddings - model.encode() - 1min 10 sec\n",
    "6. reducing dimensions  and creating best model- UMAP - 4.6 sec\n",
    "7. saving and loading embedding and dimensionality reduction model\n",
    "8. fitting model to predict - (HDBSCAN)34.4 ms sec (KMEANS) 88.9 ms\n",
    "9. KeyBERT to contribution 30 \n",
    "\"I am applying for bounce back loan\"\n",
    "apply bounce back loan,\n",
    "apply bounce_back_loan\n",
    "apply', 'bounce_back_loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   Cluster 1: \u001b[0m\n",
      "['defraud', 'fraud', 'pensioner', 'deposit', 'pension', 'fraudster', 'brazil', 'fraudulent', 'fca', 'check', 'hsbc pension', 'clawback pension', 'great pension', 'huge pension', 'brilliant pension', 'bank pension', 'people pension', 'hsbc pensioner', 'claw pension']\n",
      "\u001b[1m   Cluster 2: \u001b[0m\n",
      "['chima', 'passion', 'inspirational', 'okechukwu', 'inspiration', 'love', 'great', 'deliver', 'work', 'colour', 'love chima', 'chima inspirational', 'colour chima', 'chima great', 'chima inspiration', 'work chima', 'okechukwu chima']\n",
      "\u001b[1m   Cluster 3: \u001b[0m\n",
      "['hypocrisy', 'paradise', 'environment', 'reply', 'ignore', 'action', 'recognise', 'island', 'email', 'england', 'action hypocrisy', 'christma reply', 'environment hsbc', 'notice reply', 'like island', 'island annoying', 'mean island']\n",
      "\u001b[1m   Cluster 4: \u001b[0m\n",
      "['telephone', 'disconnect', 'phone', 'communicate', 'callback', 'replace', 'restore', 'serve', 'operate', 'register', 'telephone wait', 'phone wait', 'phone isolate', 'telephone number', 'telephone queue', 'telephone service', 'hour telephone', 'switch phone', 'help telephone']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array_text)-1):\n",
    "    clusterer=[]\n",
    "    bigramer=[]\n",
    "    print(f'\\033[1m   Cluster {i+1}: \\033[0m')\n",
    "    [clusterer.append(j) for j in unigram[i]]\n",
    "    clusterer.append(bigram[i][0])\n",
    "    \n",
    "    for word in bigram[i][0].split(\" \"):\n",
    "        if word not in bigramer:\n",
    "            bigramer.append(word)\n",
    "    \n",
    "    for each in bigram[i]:\n",
    "        first,second=each.split(\" \")\n",
    "        if first != second and not(first in bigramer and second in bigramer):\n",
    "            clusterer.append(each)\n",
    "            if bigramer.append(first): first not in bigramer\n",
    "            if bigramer.append(second): second not in bigramer\n",
    "    print(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.08395624160767\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10748c34bb5448d9a34cd200a3979d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e2aa975791049c8a7f588ed76b379c0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_39fd5938a4df4de2891cd8393dd7af2f",
      "value": " 140/140 [01:59&lt;00:00,  1.17it/s]"
     }
    },
    "39fd5938a4df4de2891cd8393dd7af2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b163e4049ca4cbe8a24e7ac79ced8a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b02fccb0afa047999665079b5248015f",
       "IPY_MODEL_10748c34bb5448d9a34cd200a3979d56"
      ],
      "layout": "IPY_MODEL_be0f028285fa4e8da720237e1b1279bf"
     }
    },
    "7e2aa975791049c8a7f588ed76b379c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b02fccb0afa047999665079b5248015f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Batches: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea4553e744634f56a3128b209cac5330",
      "max": 140,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b927e6a6b1c44be1be1f58d582da2f6a",
      "value": 140
     }
    },
    "b927e6a6b1c44be1be1f58d582da2f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "be0f028285fa4e8da720237e1b1279bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea4553e744634f56a3128b209cac5330": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
